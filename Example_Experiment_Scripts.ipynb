{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Information-Flow Clustering for 6-task experiments"
      ],
      "metadata": {
        "id": "99w1M2h_SteE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchnet"
      ],
      "metadata": {
        "id": "6o_Q6irVtH57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_id = \"IFC-6task-000\"\n",
        "\n",
        "dataset=\"MPC\"\n",
        "# dataset=\"TIC\"\n",
        "# dataset=\"KEF\"\n",
        "\n",
        "\n",
        "# arch=\"vgg16\"\n",
        "# lr,lrmin,patience,lrfactor = 0.01, 0.001, 5, 3\n",
        "# sparsity,share_sparsity_offset, numfilters, bs = 0.65, 0.05, 64, 64\n",
        "\n",
        "arch=\"modresnet18\"\n",
        "lr,lrmin,patience,lrfactor = 0.1, 0.001, 5, 5\n",
        "sparsity,share_sparsity_offset, numfilters, bs = 0.7, 0.05, 64, 64\n",
        "\n",
        "### \"structured\" performs standard IFC, while \"unstructured\" does IFC-US\n",
        "# prune_method=\"structured\"\n",
        "prune_method=\"unstructured\"\n",
        "\n",
        "# tr_epochs, ft_epochs, shared_layers, score_threshold, num_cluster_layers = 40, 30, -1, 2.0, 4\n",
        "tr_epochs, ft_epochs, shared_layers, score_threshold, num_cluster_layers = 3, 3, -1, 2.0, 4\n",
        "\n",
        "### Note: Number of subnetworks shared doesnt affect clustering or optimal manual sharing, and \"1\" is used as a placeholder value\n",
        "sim_type, num_shared, sharetype, shareorder = \"corr\", 1, \"clustering\", \"lowest\"\n",
        "# sim_type, num_shared, sharetype, shareorder = \"corr\", 1, \"optimalmanual\", \"lowest\"\n",
        "# sim_type, num_shared, sharetype, shareorder = \"corr\", 6, \"standard\", \"lowest\"\n",
        "# sim_type, num_shared, sharetype, shareorder = \"corr\", 0, \"standard\", \"lowest\"\n",
        "\n",
        "\n",
        "!python \"main.py\" --arch=$arch --dataset=$dataset --run_id=$run_id \\\n",
        "--similarity_type=$sim_type --num_shared=$num_shared --shareorder=$shareorder --share_type=$sharetype \\\n",
        "--prune_perc_per_layer=$sparsity --batch_size=$bs --train_epochs=$tr_epochs --finetune_epochs=$ft_epochs --task_order 0 1 2 3 4 5 --num_tasks=6 \\\n",
        "--task_num=0 --num_filters=$numfilters --lr=$lr --lr_patience=$patience --lr_factor=$lrfactor --lr_min=$lrmin \\\n",
        "--num_cluster_layers=$num_cluster_layers --score_threshold=$score_threshold --share_sparsity_offset=$share_sparsity_offset \\\n",
        "--shared_layers=$shared_layers --prune_method=$prune_method\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!python \"Eval T Tasks.py\" --arch=$arch --dataset=$dataset --run_id=$run_id \\\n",
        "--similarity_type=$sim_type --num_shared=$num_shared --shareorder=$shareorder --share_type=$sharetype \\\n",
        "--prune_perc_per_layer=$sparsity --batch_size=$bs --task_order 0 1 2 3 4 5 --num_tasks=6 \\\n",
        "--task_num=6 --num_filters=$numfilters --lr=$lr --lr_patience=$patience --lr_factor=$lrfactor --lr_min=$lrmin \\\n",
        "--num_cluster_layers=$num_cluster_layers --score_threshold=$score_threshold --share_sparsity_offset=$share_sparsity_offset \\\n",
        "--shared_layers=$shared_layers --prune_method=$prune_method\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LeMClQ06LB_4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ahxrugnXDz52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Two-Task Subnetwork Usefulness Experiments"
      ],
      "metadata": {
        "id": "jOCVgWu6S2t8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_id = \"IFC-2task-000\"\n",
        "\n",
        "\n",
        "dataset = \"MPC\"\n",
        "# dataset = \"TIC\"\n",
        "# dataset = \"KEF\"\n",
        "\n",
        "\n",
        "\n",
        "arch=\"vgg16\"\n",
        "lr,lrmin,patience,lrfactor = 0.01, 0.001, 5, 3\n",
        "sparsity,share_sparsity_offset, numfilters, bs = 0.65, 0.05, 64, 64\n",
        "\n",
        "# arch=\"modresnet18\"\n",
        "# lr,lrmin,patience,lrfactor = 0.1, 0.001, 5, 5\n",
        "# sparsity,share_sparsity_offset, numfilters, bs = 0.7, 0.05, 64, 64\n",
        "\n",
        "\n",
        "### Two-task experiments always train on a pair of tasks, and always share the first task's subnetwork\n",
        "num_tasks = 2\n",
        "num_shared = 1\n",
        "train_epochs, finetune_epochs, shared_layers, score_threshold, num_cluster_layers = 3, 3, -1, 2.0, 4\n",
        "\n",
        "prune_method=\"structured\"\n",
        "# prune_method=\"unstructured\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Note: Number of subnetworks shared doesnt affect clustering or optimal manual sharing, and \"1\" is used as a placeholder value\n",
        "sim_type, num_shared, sharetype, shareorder = \"corr\", 1, \"standard\", \"lowest\"\n",
        "\n",
        "\n",
        "### When task_num is 1, only task B is trained using the checkpoint from task A\n",
        "### The first time a given task A is trained, task_num must be 0, subsequent sequences we set task_num to 1\n",
        "for a in range(0,6):\n",
        "  task_num = 0\n",
        "  for b in range(0,6):\n",
        "    if b > 0:\n",
        "      task_num = 1\n",
        "\n",
        "\n",
        "    ### Runs the training for this pair of tasks a and b\n",
        "    !python \"main.py\" --arch=$arch --dataset=$dataset --run_id=$run_id \\\n",
        "    --similarity_type=$sim_type --num_shared=$num_shared --shareorder=$shareorder --share_type=$sharetype \\\n",
        "    --prune_perc_per_layer=$sparsity --batch_size=$bs --train_epochs=$train_epochs --finetune_epochs=$finetune_epochs --task_order $a $b --num_tasks=2 \\\n",
        "    --task_num=$task_num --num_filters=$numfilters --lr=$lr --lr_patience=$patience --lr_factor=$lrfactor --lr_min=$lrmin \\\n",
        "    --num_cluster_layers=$num_cluster_layers --score_threshold=$score_threshold --share_sparsity_offset=$share_sparsity_offset \\\n",
        "    --shared_layers=$shared_layers --prune_method=$prune_method\n"
      ],
      "metadata": {
        "id": "m6OEr2tVS2AC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de5c954-0ca2-42b4-dc38-a8951da732f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Weight Sharing CL/Code/CL Weight Sharing/Github/main.py\", line 22, in <module>\n",
            "    import AuxiliaryScripts.Structured.manager as manager_structured\n",
            "  File \"/content/drive/MyDrive/Weight Sharing CL/Code/CL Weight Sharing/Github/AuxiliaryScripts/Structured/manager.py\", line 14, in <module>\n",
            "    import sklearn\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/__init__.py\", line 73, in <module>\n",
            "    from .base import clone  # noqa: E402\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 19, in <module>\n",
            "    from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/__init__.py\", line 15, in <module>\n",
            "    from ._chunking import gen_batches, gen_even_slices\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_chunking.py\", line 11, in <module>\n",
            "    from ._param_validation import Interval, validate_params\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 17, in <module>\n",
            "    from .validation import _is_arraylike_not_scalar\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 21, in <module>\n",
            "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_array_api.py\", line 17, in <module>\n",
            "    from .fixes import parse_version\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/fixes.py\", line 17, in <module>\n",
            "    import scipy.stats\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/scipy/stats/__init__.py\", line 650, in <module>\n",
            "    from ._survival import *\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/scipy/stats/_survival.py\", line 219, in <module>\n",
            "    @dataclass\n",
            "     ^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/dataclasses.py\", line 1275, in dataclass\n",
            "    return wrap(cls)\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/dataclasses.py\", line 1265, in wrap\n",
            "    return _process_class(cls, init, repr, eq, order, unsafe_hash,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/dataclasses.py\", line 1092, in _process_class\n",
            "    _cmp_fn('__eq__', '==',\n",
            "  File \"/usr/lib/python3.12/dataclasses.py\", line 667, in _cmp_fn\n",
            "    return _create_fn(name,\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/dataclasses.py\", line 473, in _create_fn\n",
            "    exec(txt, globals, ns)\n",
            "  File \"<string>\", line 0, in <module>\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Weight Sharing CL/Code/CL Weight Sharing/Github/main.py\", line 17, in <module>\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 2193, in <module>\n",
            "    from torch import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nested/__init__.py\", line 21, in <module>\n",
            "    from ._internal.nested_tensor import _rebuild_njt, NestedTensor as _NestedTensor\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nested/_internal/nested_tensor.py\", line 7, in <module>\n",
            "^C\n",
            "^C\n",
            "^C\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}