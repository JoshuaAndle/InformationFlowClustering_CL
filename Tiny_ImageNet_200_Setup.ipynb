{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOeznDoqnDNS"
      },
      "source": [
        "Notebook adapted from source: https://github.com/rcamino/pytorch-notebooks/blob/master/Train%20Torchvision%20Models%20with%20Tiny%20ImageNet-200.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9WV58KyfCcF"
      },
      "source": [
        "# Prepare Tiny ImageNet-200\n",
        "\n",
        "Setting up [Tiny ImageNet-200](https://tiny-imagenet.herokuapp.com/), a subset of ImageNet with 200 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_wq4teGfkFO"
      },
      "outputs": [],
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip tiny-imagenet-200.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_5vT19BpfCcI"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl_vMRBHfCcJ"
      },
      "source": [
        "Some constants for the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kMsLXfjqfCcJ"
      },
      "outputs": [],
      "source": [
        "directory = \"./tiny-imagenet-200/\"\n",
        "num_classes = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S8IyMoQfCcJ"
      },
      "source": [
        "## Loading and pre-processing\n",
        "\n",
        "First we load and pre-process the data according to the pre-trained model [documentation](http://pytorch.org/docs/master/torchvision/models.html), applying transformations using [this example](https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L113).\n",
        "\n",
        "For all data, we keep images at their original 64x64 size, and do not apply flipping or cropping. We transform them to a tensor and finally normalize them to have values between 0 and 1. The normalization parameters come from the example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "t598_4h3fCcK",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# modify this depending on memory constraints\n",
        "batch_size = 64\n",
        "\n",
        "# the magic normalization parameters come from the example\n",
        "transform_mean = np.array([ 0.485, 0.456, 0.406 ])\n",
        "transform_std = np.array([ 0.229, 0.224, 0.225 ])\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    # transforms.Resize(32),\n",
        "    # transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = transform_mean, std = transform_std),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    # transforms.Resize(32),\n",
        "    # transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = transform_mean, std = transform_std),\n",
        "])\n",
        "\n",
        "traindir = os.path.join(directory, \"train\")\n",
        "# be careful with this set, the labels are not defined using the directory structure\n",
        "valdir = os.path.join(directory, \"val\")\n",
        "\n",
        "train = datasets.ImageFolder(traindir, train_transform)\n",
        "val = datasets.ImageFolder(valdir, val_transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "assert num_classes == len(train_loader.dataset.classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilKkSTw8fCcK"
      },
      "source": [
        "## Label madness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSar9bq_fCcK"
      },
      "source": [
        "[WordNet](https://wordnet.princeton.edu/) is a large lexical database of English. ImageNet uses a subset of this database as labels for the images, and the Tiny ImageNet-200 uses an even smaller subset. The Tiny ImageNet-200 comes with a map between WordNet ids and WordNet definitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3MA2CcbFfCcL"
      },
      "outputs": [],
      "source": [
        "small_labels = {}\n",
        "with open(os.path.join(directory, \"words.txt\"), \"r\") as dictionary_file:\n",
        "    line = dictionary_file.readline()\n",
        "    while line:\n",
        "        label_id, label = line.strip().split(\"\\t\")\n",
        "        small_labels[label_id] = label\n",
        "        line = dictionary_file.readline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqAfHdTcfCcL"
      },
      "outputs": [],
      "source": [
        "small_labels.items()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9alLL9QfCcL"
      },
      "source": [
        "The train subdirectory of Tiny ImageNet-200 has a collection of subdirectories, named using to the WordNet ids to label the images that they contain. The torchvision data loader uses the names of the subdirectories as labels, but replaces them with numeric indices when iterating the batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs_GXUBUfCcL",
        "outputId": "4c8280b6-fac3-4bc6-9fdb-ced01a37df76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['n03584254', 'n02403003', 'n02056570', 'n02769748', 'n01443537']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir(traindir)[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yiOLgib3fCcM"
      },
      "outputs": [],
      "source": [
        "labels = {}\n",
        "label_ids = {}\n",
        "for label_index, label_id in enumerate(train_loader.dataset.classes):\n",
        "    ### label_id is the string code, label is the english word(s), and label_index is the integer class number\n",
        "    label = small_labels[label_id]\n",
        "    labels[label_index] = label\n",
        "    label_ids[label_id] = label_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5ZDPcoWfCcM"
      },
      "outputs": [],
      "source": [
        "labels.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCtdCe6ffCcM"
      },
      "outputs": [],
      "source": [
        "label_ids.items()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GzpmBXhfCcM"
      },
      "source": [
        "Another problem is that the validation directory only has one subdirectory called `images`. The labels for every image inside this subdirectory are defined in a file called `val_annotations.txt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "c0v9vsLOfCcM"
      },
      "outputs": [],
      "source": [
        "val_label_map = {}\n",
        "with open(os.path.join(directory, \"val/val_annotations.txt\"), \"r\") as val_label_file:\n",
        "    line = val_label_file.readline()\n",
        "    while line:\n",
        "        file_name, label_id, _, _, _, _ = line.strip().split(\"\\t\")\n",
        "        val_label_map[file_name] = label_id\n",
        "        line = val_label_file.readline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwJVE6iTfCcM"
      },
      "outputs": [],
      "source": [
        "val_label_map.items()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwpa_Jm2fCcM"
      },
      "source": [
        "Finally we update the Tiny ImageNet-200 validation set labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iMz3ACufCcM"
      },
      "outputs": [],
      "source": [
        "val_loader.dataset.imgs[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GR0a2U1qfCcN"
      },
      "outputs": [],
      "source": [
        "for i in range(len(val_loader.dataset.imgs)):\n",
        "    file_path = val_loader.dataset.imgs[i][0]\n",
        "\n",
        "    file_name = os.path.basename(file_path)\n",
        "    label_id = val_label_map[file_name]\n",
        "\n",
        "    val_loader.dataset.imgs[i] = (file_path, label_ids[label_id])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfBrsfq_DHqg"
      },
      "outputs": [],
      "source": [
        "train_images = []\n",
        "train_labels = []\n",
        "\n",
        "for index, epoch in enumerate(train_loader):\n",
        "    # print(index)\n",
        "    images, labels = epoch\n",
        "    train_images.append(images)\n",
        "    train_labels.append(labels)\n",
        "\n",
        "# Concatenate the lists of batches into a single tensor\n",
        "train_images = torch.cat(train_images, dim=0)\n",
        "train_labels = torch.cat(train_labels, dim=0)\n",
        "\n",
        "val_images = []\n",
        "val_labels = []\n",
        "\n",
        "for index, epoch in enumerate(val_loader):\n",
        "    # print(index)\n",
        "    images, labels = epoch\n",
        "    val_images.append(images)\n",
        "    val_labels.append(labels)\n",
        "\n",
        "# Concatenate the lists of batches into a single tensor\n",
        "val_images = torch.cat(val_images, dim=0)\n",
        "val_labels = torch.cat(val_labels, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split 10% of the training data for validation and use the original validation set for labeled test data"
      ],
      "metadata": {
        "id": "faA0DTaXQ9zN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSFySce0A0BN",
        "outputId": "781e72f1-3ffe-4ba8-995b-aec976e1bb1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([100000, 3, 64, 64])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainimages.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjxHwuq5BAwa",
        "outputId": "d5f35328-8cf4-479a-b3b9-7449b84d1173"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c72e87070f0>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_-9Ti25BEvb",
        "outputId": "cc88409c-05cf-426b-ecda-5b6047a38860"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([36044, 73702, 76369,  2549, 52523, 32634, 95851, 73247, 29081, 26748])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "indices = torch.randperm(train_images.size()[0])\n",
        "\n",
        "trainimages=train_images[indices]\n",
        "trainlabels=train_labels[indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEg73kLSBYid"
      },
      "outputs": [],
      "source": [
        "valsplitimages = train_images[:10000]\n",
        "valsplitlabels = train_labels[:10000]\n",
        "trainsplitimages = train_images[10000:]\n",
        "trainsplitlabels = train_labels[10000:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WXex1jZB1tt",
        "outputId": "63917df4-14ff-44dd-b831-04181c52811f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([90000, 3, 32, 32])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainsplitimages.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9L3XN56B3gW"
      },
      "outputs": [],
      "source": [
        "torch.save(trainsplitimages, \"./X_train.pt\")\n",
        "torch.save(trainsplitlabels, \"./y_train.pt\")\n",
        "torch.save(valsplitimages, \"./X_val.pt\")\n",
        "torch.save(valsplitlabels, \"./y_val.pt\")\n",
        "torch.save(val_images, \"./X_test.pt\")\n",
        "torch.save(val_labels, \"./y_test.pt\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}